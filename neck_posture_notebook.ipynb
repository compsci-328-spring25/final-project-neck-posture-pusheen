{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this documents, we'll check if all the library has been correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- IMPORTS START --\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import datetime\n",
    "import pathlib\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from scipy import signal\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "from scipy.signal import butter, filtfilt, find_peaks\n",
    "# -- IMPORTS END --\n",
    "\n",
    "# enable zooming into graphs\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [9, 6] # width, height in inches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These helper functions exist to help avoid menually adding dictionary entries for each features every time.\n",
    "\n",
    "def row_features(feature_strs, source):\n",
    "    new_row = {}\n",
    "    for feature_str in feature_strs:\n",
    "        new_row[feature_str] = source[feature_str]\n",
    "    return new_row\n",
    "def row_features_iloc(feature_strs, source):\n",
    "    new_row = {}\n",
    "    for feature_str in feature_strs:\n",
    "        new_row[feature_str] = source.iloc[0][feature_str]\n",
    "    return new_row\n",
    "feature_list = ['avg', 'max', 'med', 'min', 'q25', 'q75', 'std', 'avg_roll', 'max_roll', 'med_roll', 'min_roll', 'q25_roll', 'q75_roll', 'std_roll', 'avg_pitch', 'max_pitch', 'med_pitch', 'min_pitch', 'q25_pitch', 'q75_pitch', 'std_pitch', 'avg_yaw', 'max_yaw', 'med_yaw', 'min_yaw', 'q25_yaw', 'q75_yaw', 'std_yaw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xyz(data):\n",
    "\n",
    "    # Set axis\n",
    "    axis = ['x', 'y', 'z']\n",
    "\n",
    "    # Generate datetime index\n",
    "    start = pd.Timestamp('2023-01-01')\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=3, nrows=1)\n",
    "\n",
    "    for i in range(len(axis)):\n",
    "\n",
    "        # Select random window\n",
    "        start = 0\n",
    "        end = 10 * 100\n",
    "        window = data.iloc[start:end]\n",
    "\n",
    "        ax = axs.flat[i]\n",
    "\n",
    "        # Plot data\n",
    "        ax.set_xticklabels([])        \n",
    "        ax.plot(window.index, window[axis[i]], label=axis[i])\n",
    "\n",
    "        # Plot peaks\n",
    "        # peak_mask = window['peaks'] != 0\n",
    "        # ax.plot(window.index[peak_mask], window['accel_mag'][peak_mask], 'ro', label='Peaks')\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set_title(f\"Axis-{axis[i]}\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize model\n",
    "def viz_tree(dt_model,features_frames,cnames):\n",
    "    # Fix feature names as list\n",
    "    feature_names = features_frames.columns.tolist()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(9,4))\n",
    "    tree.plot_tree(dt_model,\n",
    "                   feature_names=feature_names,\n",
    "                   fontsize=7,\n",
    "                   class_names=cnames,\n",
    "                   filled=True,\n",
    "                   ax=ax)\n",
    "\n",
    "    plt.title('Decision Tree')\n",
    "    plt.savefig('dt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_magnitude(data):\n",
    "\n",
    "    # Calculate magnitude\n",
    "    data['accel_mag'] = np.sqrt(data['accelerationX']**2 + data['accelerationY']**2 + data['accelerationZ']**2) # absolute accel magnitude\n",
    "    data['accel_mag'] = data['accel_mag'] - data['accel_mag'].mean() # detrend: \"remove gravity\"\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(data,sampling_rate):\n",
    "    from scipy.signal import butter, filtfilt, find_peaks\n",
    "\n",
    "    # Low pass filter\n",
    "    cutoff = 5 # Hz\n",
    "    order = 2\n",
    "    b, a = butter(order, cutoff/(sampling_rate/2), btype='lowpass')\n",
    "    data['filtered_accel_mag'] = filtfilt(b, a, data['accel_mag'])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(window):\n",
    "    features = {}\n",
    "\n",
    "    # Filtered acceleration magnitude features\n",
    "    features['avg'] = window['filtered_accel_mag'].mean()\n",
    "    features['max'] = window['filtered_accel_mag'].quantile(1)\n",
    "    features['med'] = window['filtered_accel_mag'].quantile(0.5)\n",
    "    features['min'] = window['filtered_accel_mag'].quantile(0)\n",
    "    features['q25'] = window['filtered_accel_mag'].quantile(0.25)\n",
    "    features['q75'] = window['filtered_accel_mag'].quantile(0.75)\n",
    "    features['std'] = window['filtered_accel_mag'].std()\n",
    "\n",
    "    # Roll features\n",
    "    features['avg_roll'] = window['roll'].mean()\n",
    "    features['max_roll'] = window['roll'].quantile(1)\n",
    "    features['med_roll'] = window['roll'].quantile(0.5)\n",
    "    features['min_roll'] = window['roll'].quantile(0)\n",
    "    features['q25_roll'] = window['roll'].quantile(0.25)\n",
    "    features['q75_roll'] = window['roll'].quantile(0.75)\n",
    "    features['std_roll'] = window['roll'].std()\n",
    "\n",
    "    # Pitch features\n",
    "    features['avg_pitch'] = window['pitch'].mean()\n",
    "    features['max_pitch'] = window['pitch'].quantile(1)\n",
    "    features['med_pitch'] = window['pitch'].quantile(0.5)\n",
    "    features['min_pitch'] = window['pitch'].quantile(0)\n",
    "    features['q25_pitch'] = window['pitch'].quantile(0.25)\n",
    "    features['q75_pitch'] = window['pitch'].quantile(0.75)\n",
    "    features['std_pitch'] = window['pitch'].std()\n",
    "\n",
    "    # Yaw features\n",
    "    features['avg_yaw'] = window['yaw'].mean()\n",
    "    features['max_yaw'] = window['yaw'].max()\n",
    "    features['med_yaw'] = window['yaw'].quantile(0.5)\n",
    "    features['min_yaw'] = window['yaw'].quantile(0)\n",
    "    features['q25_yaw'] = window['yaw'].quantile(0.25)\n",
    "    features['q75_yaw'] = window['yaw'].quantile(0.75)\n",
    "    features['std_yaw'] = window['yaw'].std()\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df = df._append(features,ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(frames):\n",
    "    # Extract feature columns\n",
    "    X = frames[['avg', 'max', 'med', 'min', 'q25', 'q75', 'std']]\n",
    "\n",
    "    # Extract target column\n",
    "    y = frames['activity']\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create model\n",
    "    dt_model = DecisionTreeClassifier(criterion='entropy',max_depth=5).fit(X_train, y_train)\n",
    "    dt_pred = dt_model.predict(X_test)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    acc = dt_model.score(X_test, y_test)\n",
    "    dt_cm = confusion_matrix(y_test, dt_pred, labels=dt_model.classes_)\n",
    "    print(classification_report(y_test, dt_pred))\n",
    "    print(\"Accuracy on test set:\", acc)\n",
    "\n",
    "    return dt_model,dt_cm,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract windows and features\n",
    "def extract_features(data, window_sec, sample_rate, activity):\n",
    "\twindow_str = f\"{window_sec}s\"\n",
    "\tdata['time'] = pd.to_datetime(data['time'])\n",
    "\tdata.set_index('time', inplace=True)\n",
    "\tresampled_data = data.resample(window_str)\n",
    "\tall_features = []\n",
    "\tfor time, window in resampled_data:\n",
    "\t\tfeatures = add_features(window)\n",
    "\t\tnew_row = row_features_iloc(feature_list, features)\n",
    "\t\tnew_row['activity'] = activity\n",
    "\t\tall_features.append(new_row)\n",
    "\treturn pd.DataFrame(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_data_to_combined_csv(root, output_filename = 'all_data.csv'):\n",
    "    all_data = []\n",
    "    sampling_rate = 100\n",
    "    window_sec = 5\n",
    "\n",
    "    \n",
    "    activity_folders = os.listdir(root)\n",
    "    for folder in activity_folders:\n",
    "        activity_files = glob.glob(f\"{root}/{folder}/*.csv\")\n",
    "\n",
    "        for file in activity_files:\n",
    "            df = pd.read_csv(file, parse_dates=['time'])\n",
    "            df = calc_magnitude(df)\n",
    "            df = remove_noise(df, sampling_rate)\n",
    "            df_features = extract_features(df, window_sec, sampling_rate, folder)\n",
    "\n",
    "            for index, row in df_features.iterrows():\n",
    "                new_row = row_features(feature_list, row)\n",
    "                all_data.append(new_row)\n",
    "\n",
    "    all_data = pd.DataFrame(all_data)\n",
    "    all_data.to_csv(f\"{root}/{output_filename}\")  \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Collect Training Data Into A Combined CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'std_roll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/cs328/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'std_roll'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m transform_time_to_datetime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/training_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mall_data_to_combined_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/training_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mall_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[48], line 15\u001b[0m, in \u001b[0;36mall_data_to_combined_csv\u001b[0;34m(root, output_filename)\u001b[0m\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m calc_magnitude(df)\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m remove_noise(df, sampling_rate)\n\u001b[0;32m---> 15\u001b[0m df_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_sec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df_features\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     18\u001b[0m     new_row \u001b[38;5;241m=\u001b[39m row_features(feature_list, row)\n",
      "Cell \u001b[0;32mIn[47], line 10\u001b[0m, in \u001b[0;36mextract_features\u001b[0;34m(data, window_sec, sample_rate, activity)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time, window \u001b[38;5;129;01min\u001b[39;00m resampled_data:\n\u001b[1;32m      9\u001b[0m \tfeatures \u001b[38;5;241m=\u001b[39m add_features(window)\n\u001b[0;32m---> 10\u001b[0m \tnew_row \u001b[38;5;241m=\u001b[39m \u001b[43mrow_features_iloc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \tnew_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactivity\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m activity\n\u001b[1;32m     12\u001b[0m \tall_features\u001b[38;5;241m.\u001b[39mappend(new_row)\n",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m, in \u001b[0;36mrow_features_iloc\u001b[0;34m(feature_strs, source)\u001b[0m\n\u001b[1;32m      9\u001b[0m new_row \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_str \u001b[38;5;129;01min\u001b[39;00m feature_strs:\n\u001b[0;32m---> 11\u001b[0m     new_row[feature_str] \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_str\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_row\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/cs328/lib/python3.10/site-packages/pandas/core/series.py:1121\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/cs328/lib/python3.10/site-packages/pandas/core/series.py:1237\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1237\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.0/envs/cs328/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'std_roll'"
     ]
    }
   ],
   "source": [
    "def transform_time_to_datetime(root):\n",
    "    \n",
    "    dateparse = lambda dates: [pd.to_datetime(d, unit='ns') for d in dates]\n",
    "    # Get list of all activity folders\n",
    "    activity_folders = os.listdir(root)\n",
    "    # print(activity_folders)\n",
    "\n",
    "    for folder in activity_folders:\n",
    "        # print(folder)\n",
    "        files = glob.glob(f\"{root}/{folder}/*.csv\")\n",
    "        for filename in files:\n",
    "            # print(filename)\n",
    "            df = pd.read_csv(filename, parse_dates=['time'])\n",
    "            df['time'] = pd.to_datetime(pd.to_numeric(df['time']), unit='ns')\n",
    "            df.to_csv(filename, index=False)\n",
    "warnings.filterwarnings('ignore')\n",
    "transform_time_to_datetime('data/training_data')\n",
    "\n",
    "all_data_to_combined_csv('data/training_data', 'all_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs328",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
